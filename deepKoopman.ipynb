{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch as tr\n",
    "from torch import nn\n",
    "import torch.optim as opt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchmetrics.regression import MeanSquaredError, MeanAbsoluteError\n",
    "import pandas as pd\n",
    "import matplotlib as mt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import typing\n",
    "from typing import Callable, Tuple\n",
    "import matplotlib as mt\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "option                           =3\n",
    "exp=[                            # Options: \n",
    "     \"DiscreteSpectrumExample\",  # 1)\n",
    "     \"FluidFlowBox\",             # 2)\n",
    "     \"FluidFlowOnAttractor\",     # 3)\n",
    "     \"Pendulum\"                  # 4)\n",
    "     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_initial_conditions=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if option not in [3]:\n",
    "    raise Exception(\"Option invalid, please stick with the option proposed...\")\n",
    "dataDir=\"data/\"\n",
    "match exp[option-1]:\n",
    "    case \"FluidFlowOnAttractor\":\n",
    "        max_tr       =3\n",
    "        coor         =3\n",
    "        dimEncoder   =[105,2]\n",
    "        dimAuxNet    =[300]\n",
    "        RE_IMG_eigAux=(0,1)\n",
    "        impRadialSymm=True\n",
    "        numShift     =30\n",
    "        numLinShift  =120 \n",
    "        lamb         =[0.1,0.1,1.0,1e-7]\n",
    "        lamb_l2      =1e-13\n",
    "        lr           =1e-3\n",
    "        Dt           =0.05\n",
    "        lenTrj       =121\n",
    "        batchSize    =256\n",
    "        seqEncoder=(\n",
    "                    nn.Linear(coor          ,dimEncoder[0]),nn.ReLU(),\n",
    "                    nn.Linear(dimEncoder[0] ,dimEncoder[1])\n",
    "                    )\n",
    "        seqDecoder=(\n",
    "                    nn.Linear(dimEncoder[1] ,dimEncoder[0]),nn.ReLU(),\n",
    "                    nn.Linear(dimEncoder[0] ,coor         )\n",
    "                    )\n",
    "        tmpAuxInputDim=dimEncoder[-1] if not impRadialSymm else 1\n",
    "        auxNet    =(\n",
    "                    nn.Linear(tmpAuxInputDim,dimAuxNet[0]),nn.ReLU(),\n",
    "                    nn.Linear(dimAuxNet[0]  ,1           )\n",
    "                    )\n",
    "    case EXP:\n",
    "        raise Exception(f\"Experiment Option not found.... [{EXP}]\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "filesTrain=[dataDir+exp[option-1]+\"_train\"+str(i)+\"_x.csv\" for i in range(1,max_tr+1)]\n",
    "fileVali=dataDir+exp[option-1]+\"_val_x.csv\"\n",
    "fileTest=dataDir+exp[option-1]+\"_test_x.csv\"\n",
    "print(\"Experiment\",exp[option-1]+\", files train:\")\n",
    "for f in filesTrain:\n",
    "    print(\"\\t\\t\"+f)\n",
    "print(\"\\nFile Validation:\\t\",fileVali)\n",
    "print(\"File Test:\\t\\t\",fileTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class customDataSet(Dataset):\n",
    "\n",
    "    def __init__(self, filenames: str|list[str], coor: int, Nshift: int, NLshift: int, lenTraje: int, debug: bool = False) -> None:\n",
    "        super().__init__()\n",
    "        self._data=tr.Tensor([])\n",
    "        self._Nshift =Nshift\n",
    "        self._NLshift=NLshift\n",
    "        self._maxNshift=max(Nshift,NLshift)\n",
    "        self._coor=coor\n",
    "        self._event4file=[]\n",
    "        names=[\"x_\"+str(i) for i in range(coor)]\n",
    "        multiplefiles=False\n",
    "        if isinstance(filenames,list):\n",
    "            L=len(filenames)\n",
    "            if L>1:\n",
    "                multiplefiles=True\n",
    "            else:\n",
    "                filenames=filenames[0]\n",
    "        if multiplefiles:\n",
    "            self._dataPD=pd.concat([pd.read_csv(f,header=None,names=names) for f in filenames])\n",
    "        else:\n",
    "            self._dataPD=pd.read_csv(filenames,names=names) \n",
    "        \n",
    "        if (lenTraje<=self._maxNshift):\n",
    "            raise Exception(f\"Trajectories in file is lenTraje={lenTraje} long, but you want to extract {self._maxNshift} time steps\")\n",
    "        IC=len(self._dataPD)//lenTraje\n",
    "        for i in range(IC if max_initial_conditions==None or IC<max_initial_conditions else max_initial_conditions):\n",
    "            tmp=[tr.tensor(self._dataPD[i*lenTraje+j:i*lenTraje+j+self._maxNshift+1].values).unsqueeze(0).float() for j in range(lenTraje-self._maxNshift)]\n",
    "            self._data=tr.cat((self._data,*tmp)) # This will put as first dimension the  numebr of time-shift, the second-one is the batch size and third the number of coordinates\n",
    "\n",
    "        if not debug:\n",
    "            self._dataPD=None\n",
    "\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self._data)\n",
    "\n",
    "    def __getitem__(self,i: int) -> tr.tensor:\n",
    "        return self._data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the datasets for training, validate and test the model using the files listed above  ^ ^ ^\n",
    "DATA_TR=customDataSet(filesTrain,coor,numShift,numLinShift,lenTrj)\n",
    "DATA_VL=customDataSet(fileVali  ,coor,numShift,numLinShift,lenTrj)\n",
    "DATA_TS=customDataSet(fileTest  ,coor,numShift,numLinShift,lenTrj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the datasets we create DataLoaders that group our datasets in batches\n",
    "train=DataLoader(DATA_TR,batch_size=batchSize,shuffle=True ,drop_last=True)\n",
    "vali =DataLoader(DATA_TR,batch_size=batchSize,shuffle=True ,drop_last=True)\n",
    "test =DataLoader(DATA_TR,batch_size=batchSize,shuffle=False,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train))[0:3,:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "The loss function they decided to use (and obviously the architecture itself) enforces constraints specifically designed to extract the fewest meaningful eigenfunctions; it consists of three weighted mean-squared error components: a reconstruction loss $\\mathcal{L}_{recon}$, included in order to obtain a good reconstruction accuracy of the auto-encoder, the loss future state prediction $\\mathcal{L}_{pred}$, necessary to have intrinsic coordinates that allow future state prediction,  linearity of dynamics loss $\\mathcal{L}_{lin}$, which enforces linear prediction over m time steps. They also use a $\\mathcal{L}_{\\infty}$\n",
    " term to penalty the data point with the largest loss. They also add $ℓ_2$\n",
    " regularization on the weights W to reduce overfitting. We decided to not do that because and we simply thought to make the weights smaller by adding a weight decay to Adam. However, as pointed out in [this paper](https://openreview.net/pdf?id=rk6qdGgCZ), the two things do not coincide, but we still found it effective in reducing overfitting. Their loss is:\n",
    " $$ \\mathcal{L} = \\alpha_1 (\\mathcal{L}_{recon} + \\mathcal{L}_{pred}) + \\mathcal{L}_{lin} + \\alpha_2 \\mathcal{L}_{\\infty} + \\alpha_3 ||\\textbf{W}||^2_2$$\n",
    " \n",
    " $$ \\mathcal{L}_{recon} = ||\\textbf{x}_1 - \\varphi^{-1}(\\varphi(\\textbf{x}_1))||_{\\text{MSE}}$$\n",
    " $$ \\mathcal{L}_{pred} = \\frac{1}{S_p} \\sum_{m=1}^{S_p} ||\\textbf{x}_{m+1} -  \\varphi^{-1}(K^m \\varphi(\\textbf{x}_1))||_{\\text{MSE}}$$\n",
    " $$ \\mathcal{L}_{lin} = \\frac{1}{T - 1} \\sum_{m=1}^{T-1} ||\\varphi(\\textbf{x}_{m+1}) - K^m \\varphi(\\textbf{x}_1)||_{\\text{MSE}}$$\n",
    "  $$ \\mathcal{L}_{\\infty} = ||\\textbf{x}_1 - \\varphi^{-1}(\\varphi(\\textbf{x}_1))||_{\\infty} + ||\\textbf{x}_2 - \\varphi^{-1}(K\\varphi(\\textbf{x}_1))||_{\\infty}$$\n",
    "  where $\\text{MSE}$ refers to mean squared error and T is the number of time steps in each trajectory. The weights $\\alpha_1$, $\\alpha_2$, and $\\alpha_3$ are hyperparameters which are different for each example. The integer $S_p$ is a hyperparameter for how many steps to check in the prediction loss. In our case we do not have the term proportional to $|| W ||_2^2|| $ and the weight decay was set equal to $\\alpha_3$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self,Nshift: int, NLshift: int) -> None:\n",
    "        super().__init__()\n",
    "        #self._Nshift =Nshift\n",
    "        #self._NLshift=NLshift\n",
    "        self.loss_reco=nn.MSELoss()\n",
    "        #self.loss_pred=nn.MSELoss()\n",
    "        #self.loss_line=nn.MSELoss()\n",
    "        #self.loss_linf: Callable[[tr.tensor,tr.tensor,tr.tensor,tr.tensor],tr.tensor]=\\\n",
    "        #    lambda x,y,x1,y1: ((x-y).abs().max(dim=-1)[0]+(x1-y1).abs().max(dim=-1)[0]).mean()\n",
    "\n",
    "    #XT contains the true time-evolved starting from the initials points of trajectories in a batch, YT the predicted ones.\n",
    "    def forward(self, XT: tr.tensor, YT: tr.tensor, phiT: tr.tensor, phiPredT: tr.tensor) -> tr.tensor:\n",
    "\n",
    "        totLoss=        self.loss_reco(XT[0],YT[0]) * lamb[0]\n",
    "        # totLoss=totLoss+self.loss_pred(XT[1:self._Nshift+1],YT[1:self._Nshift+1])           * lamb[1]\n",
    "        # totLoss=totLoss+self.loss_line(phiT[1:self._NLshift+1],phiPredT[1:self._NLshift+1]) * lamb[2]\n",
    "        # totLoss=totLoss+self.loss_linf(XT[0],YT[0],XT[1],YT[1])                             * lamb[3]\n",
    "\n",
    "        return  totLoss\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metric   = MeanSquaredError()\n",
    "metric   = MeanAbsoluteError() # As metric we used the MAE between a point evolved one time step in the future and the correspondent prediction done by the Net\n",
    "loss     = CustomLoss(numShift,numLinShift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.layers=nn.Sequential(*seqEncoder)\n",
    "    def forward(self,x: tr.tensor) -> tr.tensor:\n",
    "        return self.layers(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.layers=nn.Sequential(*seqDecoder)\n",
    "    def forward(self,x: tr.tensor) -> tr.tensor:\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "#==================================================================================================\n",
    "#--------------------------------------------------------------------------------------------------\n",
    "#==================================================================================================\n",
    "\n",
    "class K_Matrix(nn.Module):\n",
    "    '''\n",
    "    `K_Matrix` is the realization of the whole process of fixing the continous parameters and applying the matrix. It is responsible of the evolution\n",
    "    in time. It acts on the Koopman subspace.\n",
    "    If it is initialized with `radialS,radialSymmetry: bool =Falseymmetry` setted to True, the matrix eigenvalues will be calculated from vector lenght information (not direction)\n",
    "    '''\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.auxNet=nn.Sequential(nn.Linear(2,300),nn.ReLU(),\n",
    "                                  nn.Linear(300,2)          )\n",
    "        self.M=None\n",
    "    def forward(self,x,shift):\n",
    "        # We compose the Jordan matrix fixing the real parameters\n",
    "        PR=self.auxNet(x).T\n",
    "        R=tr.exp(PR[0]*Dt)\n",
    "        C=tr.cos(PR[1]*Dt)\n",
    "        S=tr.sin(PR[1]*Dt)\n",
    "        M=tr.cat([R*C,R*S,\n",
    "                 -R*S,R*C]).reshape(2,2,-1).permute(2,0,1)\n",
    "        self.M=M.clone()\n",
    "        # We evolve in time applying the matrix product between the point in Koopman subspace and the Jordan matrix\n",
    "        M=tr.linalg.matrix_power(M,shift)\n",
    "        x=x.unsqueeze(-1)\n",
    "        res=M.bmm(x).squeeze(-1) # N.B. the matrix is shaped (`batchSize`,`n`,`n`) with `n` the dimension of the Koopman subspace.\n",
    "                                               #      to perform batch matrix multplication we had to tranform the vector in a `n`x 1 matrix \n",
    "                                               #      (`batchSize`,`n`,1). Than we had to squeeze back the result in vectorial form (`batchSize`,`n`)\n",
    "        return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nt=max(numShift,numLinShift)\n",
    "print(f\"Numero di predizioni che il modello compie: {Nt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KOOPMAN(nn.Module):\n",
    "    def __init__(self, pred: int ,radialSymmetry: bool =False) -> None:\n",
    "        super().__init__()\n",
    "        self.ENC=Encoder()\n",
    "        self.K=K_Matrix()\n",
    "        self.Npred=pred\n",
    "        self.DEC=Decoder()\n",
    "    def forward(self,XT: tr.tensor,NShift: int) -> Tuple[tr.tensor,tr.tensor,tr.tensor,tr.tensor]:\n",
    "        PhiT=self.ENC(XT.reshape(-1,coor))\n",
    "        arr=[]\n",
    "        for i in range(self.Npred+1):\n",
    "            arr.append(self.K(PhiT,i))\n",
    "        PhiPredT=tr.cat(arr)\n",
    "        YT=self.DEC(PhiPredT).reshape(self.Npred+1,len(XT),batchSize,coor)\n",
    "        return XT, YT, PhiT.reshape(len(XT),batchSize,dimEncoder[-1]),PhiPredT.reshape(self.Npred+1,len(XT),batchSize,dimEncoder[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=KOOPMAN(T)\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Procedure\n",
    "\n",
    "They initialized each weight matrix W randomly from a uniform distribution in the range $[-s, s]$ for $s = \\frac{1}{\\sqrt{a}}$, where $a$ is the dimension of the input of the layer. Each bias vector $b$ is initialized to 0. The learning rate for the Adam optimizer is 0.001. We also use early stopping; for each model, at the end of training, we resume the step with the lowest validation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=100  # Number of epochs for the training\n",
    "patience=20 # If after `patience` epochs the model did not improve, the training is automatically stopped\n",
    "lr=0.01\n",
    "lamb_l2=0\n",
    "optim= opt.Adam(params=model.parameters(),lr=lr,weight_decay=lamb_l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossRECO(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.loss_reco=nn.MSELoss()\n",
    "        \n",
    "\n",
    "    def forward(self, XT: tr.tensor, YT: tr.tensor) -> tr.tensor:\n",
    "\n",
    "        totLoss=        self.loss_reco(XT,YT[0])\n",
    "        return  totLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossPRED(nn.Module):\n",
    "    def __init__(self,Nshift) -> None:\n",
    "        super().__init__()\n",
    "        self.loss_pred=nn.MSELoss()\n",
    "        self.loss_array=nn.MSELoss(reduction='none')\n",
    "        self.NT=Nshift\n",
    "\n",
    "    def forward(self, XT: tr.tensor, YT: tr.tensor) -> tr.tensor:\n",
    "        totLoss=        self.loss_pred(XT[1:self.NT+1],YT[1:self.NT+1,0])\n",
    "        return  totLoss, self.loss_array(XT[1:self.NT+1],YT[1:self.NT+1,0]).mean(dim=(-2,-1))\n",
    "\n",
    "class LossPRED_PHI(nn.Module):\n",
    "    def __init__(self,Nshift) -> None:\n",
    "        super().__init__()\n",
    "        self.loss_pred=nn.MSELoss()\n",
    "        self.loss_array=nn.MSELoss(reduction='none')\n",
    "        self.NT=Nshift\n",
    "\n",
    "    def forward(self, phiT: tr.tensor, phiPredT: tr.tensor) -> tr.tensor:\n",
    "\n",
    "        totLoss=        self.loss_pred(phiT[1:self.NT+1],phiPredT[1:self.NT+1,0])\n",
    "        return  totLoss, self.loss_array(phiT[1:self.NT+1],phiPredT[1:self.NT+1,0]).mean(dim=(-2,-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping(Exception):\n",
    "    '''\n",
    "    Custom exception to raise in case of stop due to number of epochs in which the model did not improve greater than `patience`.\n",
    "    '''\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "    \n",
    "\n",
    "\n",
    "class SaveBestModel:\n",
    "    '''\n",
    "    This utility is needed to save the best model (choosen using the one that minimizes the loss on the validation dataset) and to check if\n",
    "    any improvement as occurred during last epochs.\n",
    "    `best_valid_loss` is the minimum of the loss found\n",
    "    `best_epoch`      is the epoch at witch the validation was minimal\n",
    "    `patience`        is the maximum number of epochs that we consider the model to have the potential of improving over the last updating\n",
    "    '''\n",
    "    def __init__(self,patience=100, best_vali_loss=float('inf')) -> None: #object initialized with best_loss = +∞\n",
    "        self.best_vali_loss = best_vali_loss\n",
    "        self.patience=patience\n",
    "        self.best_epoch=0\n",
    "        self._fromLastUpdate=0\n",
    "        \n",
    "    def __call__(\n",
    "        self, current_vali_loss: float, epoch: int,\n",
    "        model: nn.Module, \n",
    "        optimizer: opt.Optimizer,\n",
    "        criterion: Callable[[tr.tensor,tr.tensor],float], \n",
    "        metric: float\n",
    "    ):\n",
    "        if current_vali_loss < self.best_vali_loss:\n",
    "            # Private attribute containing the last epochs in which the model improved\n",
    "            self._fromLastUpdate=0\n",
    "            self.best_vali_loss = current_vali_loss\n",
    "            print(f\"\\nBest validation loss: {self.best_vali_loss}\")\n",
    "            print(f\"\\nSaving best model for epoch: {epoch+1}\\n\")\n",
    "            # method to save a model (the state_dict: a python dictionary object that \n",
    "            # maps each layer to its parameter tensor) and other useful parametrers\n",
    "            # see: https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "            tr.save({\n",
    "                     'epoch': epoch+1,\n",
    "                     'model_state_dict': model.state_dict(),\n",
    "                     'optimizer_state_dict': optimizer.state_dict(),\n",
    "                     'loss': criterion,\n",
    "                     'metric': metric\n",
    "                    },\n",
    "                    'best_model.pth')\n",
    "            self.best_epoch=epoch+1\n",
    "        else:\n",
    "            self._fromLastUpdate+=1\n",
    "\n",
    "        # Trow an exception if model has to stop due to early stopping\n",
    "        if self._fromLastUpdate>self.patience:\n",
    "            raise EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_best_model=SaveBestModel(patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=10\n",
    "loss0=LossRECO()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_best_model=SaveBestModel(patience)\n",
    "# Lists containing history during epochs of:\n",
    "h_loss_tr  =[] # loss on train dataset\n",
    "h_metric_tr=[] # metric on train dataset\n",
    "h_loss_vl  =[] # loss on validation dataset\n",
    "h_metric_vl=[] # metric on validation dataset\n",
    "\n",
    "metric.to(device)\n",
    "model=model.to(device)\n",
    "for e in range(EPOCHS):\n",
    "    t0=time.time()\n",
    "    # Put model in train mode\n",
    "    model.train()\n",
    "    lossVal=0.0\n",
    "    metrVal=0.0\n",
    "    nbatch=0\n",
    "    # Loop for each batch in all the train dataset\n",
    "    for x in train:\n",
    "        XT=x.to(device).swapaxes(0,1)\n",
    "        nbatch+=1\n",
    "        XT, YT, PhiT,PhiPredT=model(XT,Nt)\n",
    "        l=loss0(XT, YT)\n",
    "        m=0.#metric(XT[1],YT[1])\n",
    "        l.backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "        lossVal+=l.item()\n",
    "        metrVal+=0.#m.item()\n",
    "        \n",
    "    # Normalize the loss and the metric to have the mean value between all the batches\n",
    "    lossVal/=nbatch\n",
    "    metrVal/=nbatch\n",
    "    h_loss_tr.append(lossVal)\n",
    "    h_metric_tr.append(metrVal)\n",
    "\n",
    "    # Put model in evaluation mode\n",
    "    model.eval()\n",
    "    vl_lossVal=0.0\n",
    "    vl_metrVal=0.0\n",
    "    nbatch=0\n",
    "    # Loop for each batch in all the validation dataset\n",
    "    for x in vali:\n",
    "        nbatch+=1\n",
    "        XT=x.to(device).swapaxes(0,1)\n",
    "        XT, YT, PhiT,PhiPredT=model(XT,Nt)\n",
    "        l=loss0(XT, YT)\n",
    "        m=0.#metric(XT[1],YT[1])\n",
    "        vl_lossVal+=l.item()\n",
    "        vl_metrVal+=0.#m.item()\n",
    "    vl_lossVal/=nbatch\n",
    "    vl_metrVal/=nbatch\n",
    "    h_loss_vl.append(vl_lossVal)\n",
    "    h_metric_vl.append(vl_metrVal)\n",
    "\n",
    "    elapsed_time = time.time()-t0    \n",
    "    print(f\"epoch: {e+1}, time(s): {elapsed_time:.2f}, train loss: {lossVal:.6f}, train metric: {metrVal:.6f}, vali loss: {vl_lossVal:.6f}, vali metric: {vl_metrVal:.6f}\")\n",
    "    try: \n",
    "        save_best_model(vl_lossVal,e,model,optim,metric,metrVal)\n",
    "    except EarlyStopping:\n",
    "        print(f\"Early Stopping occurred at epoch: {e+1}, best validation loss is {save_best_model.best_vali_loss}\")\n",
    "        break\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch=save_best_model.best_epoch\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(range(1,len(h_loss_tr)+1),   h_loss_tr, color='green', linestyle='-', label='train loss')\n",
    "plt.plot(range(1,len(h_loss_vl)+1),  h_loss_vl, color='blue', linestyle='-', label='validation loss')\n",
    "plt.axvline(best_epoch,color=\"r\",linestyle=\"--\",label=\"best epoch\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(range(1,len(h_metric_tr)+1),  h_metric_tr,  color='green', linestyle='-', label='train metric')\n",
    "plt.plot(range(1,len(h_metric_vl)+1),  h_metric_vl, color='blue', linestyle='-', label='validation metric')\n",
    "plt.axvline(best_epoch,color=\"r\",linestyle=\"--\",label=\"best epoch\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Metric')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=100\n",
    "T=20\n",
    "loss1=LossPRED(T)\n",
    "loss2=LossPRED_PHI(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.0002\n",
    "lamb_l2=0\n",
    "optim= opt.Adam(params=model.parameters(),lr=lr,weight_decay=lamb_l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_best_model=SaveBestModel(patience)\n",
    "# Lists containing history during epochs of:\n",
    "h_loss_tr  =[] # loss on train dataset\n",
    "h_metric_tr=[] # metric on train dataset\n",
    "h_loss_vl  =[] # loss on validation dataset\n",
    "h_metric_vl=[] # metric on validation dataset\n",
    "\n",
    "H_LOSS_VL=[]\n",
    "\n",
    "\n",
    "metric.to(device)\n",
    "model=model.to(device)\n",
    "maxShift=max(numShift,numLinShift) # The model will provide prediction to fulfill the bigger requirement in term of \n",
    "                                   # time-shift in the future, then the custom loss use only the needed time-spans\n",
    "for e in range(EPOCHS):\n",
    "    t0=time.time()\n",
    "    # Put model in train mode\n",
    "    model.train()\n",
    "    lossVal=0.0\n",
    "    metrVal=0.0\n",
    "    nbatch=0\n",
    "    # Loop for each batch in all the train dataset\n",
    "    for x in train:\n",
    "        XT=x.to(device).swapaxes(0,1)\n",
    "        nbatch+=1\n",
    "        XT, YT, PhiT,PhiPredT=model(XT,Nt)\n",
    "        l1,lA1=loss1(XT, YT)\n",
    "        l2,lA2=loss2(PhiT,PhiPredT)\n",
    "        lA=lA1+lA2\n",
    "       \n",
    "        l=l1+l2#+loss0(XT,YT)\n",
    "        m=0.#metric(XT[1],YT[1])\n",
    "        l.backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "        lossVal+=l.item()\n",
    "        metrVal+=0.#m.item()\n",
    "        \n",
    "    # Normalize the loss and the metric to have the mean value between all the batches\n",
    "    lossVal/=nbatch\n",
    "    metrVal/=nbatch\n",
    "\n",
    "    h_loss_tr.append(lossVal)\n",
    "    h_metric_tr.append(metrVal)\n",
    "\n",
    "    # Put model in evaluation mode\n",
    "    model.eval()\n",
    "    vl_lossVal=0.0\n",
    "    vl_metrVal=0.0\n",
    "    nbatch=0\n",
    "    # Loop for each batch in all the validation dataset\n",
    "    lA=0.\n",
    "    for x in vali:\n",
    "        nbatch+=1\n",
    "        x=x.to(device).swapaxes(0,1)\n",
    "        XT, YT, PhiT,PhiPredT=model(x,Nt)\n",
    "        l1,lA1=loss1(XT, YT)\n",
    "        l2,lA2=loss2(PhiT,PhiPredT)\n",
    "        lA=lA+lA1+lA2\n",
    "        l=l1+l2*0.#+loss0(XT,YT)\n",
    "        #m=metric(XT[1],YT[1])\n",
    "        vl_lossVal+=l.item()\n",
    "        vl_metrVal+=0.#m.item()\n",
    "    vl_lossVal/=nbatch\n",
    "    vl_metrVal/=nbatch\n",
    "    H_LOSS_VL.append(lA.detach().numpy()/nbatch)\n",
    "    h_loss_vl.append(vl_lossVal)\n",
    "    h_metric_vl.append(vl_metrVal)\n",
    "\n",
    "    elapsed_time = time.time()-t0    \n",
    "    print(f\"epoch: {e+1}, time(s): {elapsed_time:.2f}, train loss: {lossVal:.6f}, train metric: {metrVal:.6f}, vali loss: {vl_lossVal:.6f}, vali metric: {vl_metrVal:.6f}\")\n",
    "    try: \n",
    "        save_best_model(vl_lossVal,e,model,optim,metric,metrVal)\n",
    "    except EarlyStopping:\n",
    "        print(f\"Early Stopping occurred at epoch: {e+1}, best validation loss is {save_best_model.best_vali_loss}\")\n",
    "        break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_LOSS_VL_PLOT=np.stack(H_LOSS_VL).T\n",
    "for i,h in enumerate(H_LOSS_VL_PLOT):\n",
    "    plt.plot(list(range(1,len(h)+1)),h,label=str(i))\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.K(model.ENC(XT[0]),1)\n",
    "print(model.K.M[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch=save_best_model.best_epoch\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(range(1,len(h_loss_tr)+1),   h_loss_tr, color='green', linestyle='-', label='train loss')\n",
    "plt.plot(range(1,len(h_loss_vl)+1),  h_loss_vl, color='blue', linestyle='-', label='validation loss')\n",
    "plt.axvline(best_epoch,color=\"r\",linestyle=\"--\",label=\"best epoch\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(range(1,len(h_metric_tr)+1),  h_metric_tr,  color='green', linestyle='-', label='train metric')\n",
    "plt.plot(range(1,len(h_metric_vl)+1),  h_metric_vl, color='blue', linestyle='-', label='validation metric')\n",
    "plt.axvline(best_epoch,color=\"r\",linestyle=\"--\",label=\"best epoch\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Metric')\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cpu()\n",
    "metric.cpu()\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "ts_lossVal=0.0\n",
    "ts_metrVal=0.0\n",
    "nbatch=0\n",
    "maxShift=max(numShift,numLinShift)\n",
    "# Loop for each batch in all the test dataset\n",
    "for x in test:\n",
    "    nbatch+=1\n",
    "    x=x.cpu().swapaxes(0,1)\n",
    "    XT, YT, PhiT,PhiPredT=model(x,0)\n",
    "    #l=loss(XT, YT, PhiT,PhiPredT)\n",
    "    m=0.#metric(XT[1],YT[1])\n",
    "    ts_lossVal+=0#l.item()\n",
    "    ts_metrVal+=0.#m.item()\n",
    "ts_lossVal/=nbatch\n",
    "ts_metrVal/=nbatch\n",
    "print(f\"Loss value on TEST dataset:\\t{ts_lossVal}\\nMetric on TEST dataset:\\t\\t{ts_metrVal}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs\n",
    "\n",
    "In order to test our model we considered the following plots:\n",
    "\n",
    "1) The correlation $ \\langle \\textbf{x}(k) , \\textbf{x}(k+ m) \\rangle $ at different discrete time steps m (evolution provided by the network) and fixed k and we compared it with the result found with numerical integration.\n",
    "2) $\\textbf{x}_i (k)\\ \\text{vs}\\ \\textbf{x}_j (k)$ at different time steps k (evolution provided by the network)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_corr_m=XT[:T+1].permute((2,0,1)).cpu().detach().numpy()\n",
    "Y_corr_m=YT[:T+1,0,:].permute((2,0,1)).cpu().detach().numpy()\n",
    "\n",
    "#plt.rcParams['text.usetex'] = True\n",
    "plt.rcParams['text.usetex']=False\n",
    "fig,axs=plt.subplots(1,3,figsize=(15,5))\n",
    "ax1,ax2,ax3=axs\n",
    "ax1.set_title(\"Auto correlation of trajectory \\$\\\\langle \\\\textbf(k) , \\textbf\\{x\\}(k+ m) \\\\rangle$\")\n",
    "ax1.set_xlabel(\"Time-steps difference in frames m\")\n",
    "ax1.set_ylabel(\"Pearson coefficient\")\n",
    "ax2.set_title(\"Corr. true and predicted trajectory \\$\\\\langle \\\\textbf(k+m) , \\\\textbf{y(k+ m) \\\\rangle$\")\n",
    "ax2.set_xlabel(\"Time-steps difference in frames $m$\")\n",
    "ax2.set_ylabel(\"Pearson coefficient\")\n",
    "ax3.set_title(\"Auto correlation of predicted trajectory \\$\\\\langle \\\\textbf\\{y\\}(k) , \\\\textbf\\{y\\}(k+ m) \\\\rangle$\")\n",
    "ax3.set_xlabel(\"Time-steps difference in frames $m$\")\n",
    "ax3.set_ylabel(\"Pearson coefficient\")\n",
    "\n",
    "for i in range(coor):\n",
    "    X=range(1,len(X_corr_m[i])+1)\n",
    "    Y=[]\n",
    "    for ix in range(len(X)):\n",
    "        Y.append(pearsonr(X_corr_m[i][0],X_corr_m[i][ix])[0])\n",
    "    ax1.plot(X,Y,label=\"$\\\\langle x_\"+str(i)+\"(k), x_\"+str(i)+\"(k+m) \\\\rangle$\",marker='.')\n",
    "    Y=[]\n",
    "    for ix in range(len(X)):\n",
    "        Y.append(pearsonr(X_corr_m[i][ix],Y_corr_m[i][ix])[0])\n",
    "    ax2.plot(X,Y,label=\"$\\\\langle x_\"+str(i)+\"(k+m), y_\"+str(i)+\"(k+m) \\\\rangle$\",marker='.')\n",
    "    Y=[]\n",
    "    for ix in range(len(X)):\n",
    "        Y.append(pearsonr(Y_corr_m[i][0],Y_corr_m[i][ix])[0])\n",
    "    ax3.plot(X,Y,label=\"$\\\\langle y_\"+str(i)+\"(k), y_\"+str(i)+\"(k+m) \\\\rangle$\",marker='.')\n",
    "\n",
    "ax1.legend()\n",
    "ax2.legend()\n",
    "ax3.legend()\n",
    "fig.savefig(\"correlations.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAN=np.random.randint(0,len(XT[0].cpu().detach().numpy()))\n",
    "X_traj=XT[:T+1].permute((1,2,0)).cpu().detach().numpy()[RAN]\n",
    "Y_traj=YT[:T+1,0].permute((1,2,0)).cpu().detach().numpy()[RAN]\n",
    "\n",
    "fig,ax=plt.subplots(1,figsize=(5,5))\n",
    "ax.set_title(f\"Trajectory in phase space [initial condition {RAN}]\")\n",
    "ax.set_xlabel(r\"$x_0(t)$\")\n",
    "ax.set_ylabel(r\"$x_1(t)$\")\n",
    "ax.plot(X_traj[0],X_traj[1],label=\"Trajectory\",marker='.')\n",
    "ax.plot(Y_traj[0],Y_traj[1],label=\"Predicted\",marker='.')\n",
    "ax.legend()\n",
    "fig.savefig(\"random_trajectory.png\")\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
